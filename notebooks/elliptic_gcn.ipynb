{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import networkx as nx;\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_classes = pd.read_csv(\"elliptic_txs_classes.csv\")\n",
    "df_edgelist = pd.read_csv(\"elliptic_txs_edgelist.csv\")\n",
    "df_features = pd.read_csv(\"elliptic_txs_features.csv\", header=None)\n",
    "\n",
    "# Rename feature columns\n",
    "df_features.columns = ['txId', 'timeStep'] + [f'localV{i}' for i in range(94)] + [f'aggregatedV{i}' for i in range(71)]\n",
    "\n",
    "# Class mapping\n",
    "class_mapping = {\n",
    "    '2': 0,         # licit\n",
    "    '1': 1,         # illicit\n",
    "    'unknown': 2    # unknown\n",
    "}\n",
    "df_classes['class'] = df_classes['class'].map(class_mapping)\n",
    "\n",
    "# txId mapping\n",
    "txId_mapping = {txId: idx for idx, txId in enumerate(df_features['txId'].unique())}\n",
    "df_classes['txId'] = df_classes['txId'].map(txId_mapping)\n",
    "df_edgelist = df_edgelist.apply(lambda col: col.map(txId_mapping))\n",
    "df_features['txId'] = df_features['txId'].map(txId_mapping)\n",
    "\n",
    "# Merge features and classes\n",
    "df = pd.merge(df_classes, df_features, on='txId')\n",
    "\n",
    "# Filter out unknown classes\n",
    "df = df[df['class'] != 2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by timeStep\n",
    "time_steps = df['timeStep'].unique()\n",
    "num_time_steps = len(time_steps)\n",
    "\n",
    "# Determine split points for time steps\n",
    "train_end = int(0.7 * num_time_steps)   # 70% for training\n",
    "\n",
    "# Train test split based on time steps\n",
    "train_time_steps = time_steps[:train_end]\n",
    "test_time_steps = time_steps[train_end:]\n",
    "\n",
    "train_mask = df['timeStep'].isin(train_time_steps)\n",
    "test_mask = df['timeStep'].isin(test_time_steps)\n",
    "\n",
    "train_df = df[train_mask].reset_index(drop=True)\n",
    "test_df = df[test_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   6.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   6.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=  12.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   7.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   5.1s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=  13.3s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   2.0s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.0s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   7.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  14.9s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   7.6s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=  14.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   5.1s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   7.0s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   9.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=  12.1s\n",
      "Best Hyperparameters:\n",
      "{'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "train_x = train_df.drop(columns=['txId', 'timeStep', 'class']).values\n",
    "train_y = train_df['class'].values\n",
    "test_x = test_df.drop(columns=['txId', 'timeStep', 'class']).values\n",
    "test_y = test_df['class'].values\n",
    "\n",
    "# TimeSeriesSplit for temporal CV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='f1',  # You can also try 'f1_macro' or 'f1_weighted'\n",
    "    n_jobs=-1,     # Use all cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run grid search on train_x\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "# Best estimator and params\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Evaluation on Test Set (Best Model) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       licit       0.98      1.00      0.99     15587\n",
      "     illicit       0.97      0.72      0.83      1083\n",
      "\n",
      "    accuracy                           0.98     16670\n",
      "   macro avg       0.97      0.86      0.91     16670\n",
      "weighted avg       0.98      0.98      0.98     16670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_rf.predict(test_x)\n",
    "print(\"=== Final Evaluation on Test Set (Best Model) ===\")\n",
    "print(classification_report(test_y, test_preds, target_names=[\"licit\", \"illicit\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolution Network (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 203769\n",
      "Number of features: 165\n",
      "Number of edges: 234355\n",
      "Class distribution: tensor([ 42019,   4545, 157205])\n",
      "Train class distribution: tensor([26432,  3462])\n",
      "Validation class distribution: tensor([5039,  447])\n",
      "Test class distribution: tensor([10548,   636])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = Data(\n",
    "    x=torch.tensor(df_features.drop(columns=['txId', 'timeStep']).values, dtype=torch.float32),\n",
    "    edge_index=torch.tensor(df_edgelist.values.T, dtype=torch.long),\n",
    "    y=torch.tensor(df_classes['class'].values, dtype=torch.long)\n",
    ")\n",
    "\n",
    "data = data.to(device)\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Class distribution: {data.y.bincount()}\")\n",
    "\n",
    "# Filter out unknown classes (class 2)\n",
    "known_mask = (data.y != 2)\n",
    "filtered_x = data.x[known_mask]\n",
    "filtered_y = data.y[known_mask]\n",
    "filtered_features = df_features.iloc[known_mask.cpu().numpy()]\n",
    "\n",
    "# Reset the index of filtered_features to align with filtered_y\n",
    "filtered_features = filtered_features.reset_index(drop=True)\n",
    "\n",
    "# Group data by timeStep\n",
    "time_steps = filtered_features['timeStep'].unique()\n",
    "num_time_steps = len(time_steps)\n",
    "\n",
    "# Determine split points for time steps\n",
    "train_end = int(0.7 * num_time_steps)  # 70% for training\n",
    "val_end = int(0.8 * num_time_steps)   # 10% for validation (70% + 10%)\n",
    "\n",
    "# Split time steps into train, validation, and test sets\n",
    "train_time_steps = time_steps[:train_end]\n",
    "val_time_steps = time_steps[train_end:val_end]\n",
    "test_time_steps = time_steps[val_end:]\n",
    "\n",
    "# Create masks based on time steps\n",
    "train_mask = torch.tensor(\n",
    "    filtered_features[filtered_features['timeStep'].isin(train_time_steps)].index.values, dtype=torch.long\n",
    ")\n",
    "val_mask = torch.tensor(\n",
    "    filtered_features[filtered_features['timeStep'].isin(val_time_steps)].index.values, dtype=torch.long\n",
    ")\n",
    "test_mask = torch.tensor(\n",
    "    filtered_features[filtered_features['timeStep'].isin(test_time_steps)].index.values, dtype=torch.long\n",
    ")\n",
    "\n",
    "# Extract train, validation, and test labels\n",
    "train_y = filtered_y[train_mask]\n",
    "val_y = filtered_y[val_mask]\n",
    "test_y = filtered_y[test_mask]\n",
    "\n",
    "# Print class distributions\n",
    "print(f\"Train class distribution: {train_y.bincount()}\")\n",
    "print(f\"Validation class distribution: {val_y.bincount()}\")\n",
    "print(f\"Test class distribution: {test_y.bincount()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(165, 16)\n",
      "  (conv2): GCNConv(16, 2)\n",
      ")\n",
      "Epoch 1, Loss: 0.6365, Train Acc: 0.8588, Train F1: 0.8217, Train AUC: 0.4963, Val Acc: 0.8956, Val F1: 0.8713, Val AUC: 0.5155\n",
      "Epoch 2, Loss: 0.4464, Train Acc: 0.8764, Train F1: 0.8275, Train AUC: 0.5103, Val Acc: 0.9100, Val F1: 0.8768, Val AUC: 0.5177\n",
      "Epoch 3, Loss: 0.3996, Train Acc: 0.8821, Train F1: 0.8293, Train AUC: 0.5235, Val Acc: 0.9141, Val F1: 0.8780, Val AUC: 0.5113\n",
      "Epoch 4, Loss: 0.3997, Train Acc: 0.8830, Train F1: 0.8295, Train AUC: 0.5327, Val Acc: 0.9149, Val F1: 0.8784, Val AUC: 0.5020\n",
      "Epoch 5, Loss: 0.4091, Train Acc: 0.8782, Train F1: 0.8274, Train AUC: 0.5300, Val Acc: 0.9143, Val F1: 0.8781, Val AUC: 0.4930\n",
      "Epoch 6, Loss: 0.4207, Train Acc: 0.8828, Train F1: 0.8297, Train AUC: 0.5449, Val Acc: 0.9154, Val F1: 0.8783, Val AUC: 0.4811\n",
      "Epoch 7, Loss: 0.4124, Train Acc: 0.8831, Train F1: 0.8298, Train AUC: 0.5497, Val Acc: 0.9154, Val F1: 0.8783, Val AUC: 0.4713\n",
      "Epoch 8, Loss: 0.4050, Train Acc: 0.8837, Train F1: 0.8301, Train AUC: 0.5537, Val Acc: 0.9162, Val F1: 0.8787, Val AUC: 0.4666\n",
      "Epoch 9, Loss: 0.3948, Train Acc: 0.8840, Train F1: 0.8303, Train AUC: 0.5574, Val Acc: 0.9171, Val F1: 0.8791, Val AUC: 0.4643\n",
      "Epoch 10, Loss: 0.3850, Train Acc: 0.8840, Train F1: 0.8304, Train AUC: 0.5602, Val Acc: 0.9172, Val F1: 0.8792, Val AUC: 0.4637\n",
      "Epoch 11, Loss: 0.3782, Train Acc: 0.8840, Train F1: 0.8304, Train AUC: 0.5622, Val Acc: 0.9169, Val F1: 0.8790, Val AUC: 0.4640\n",
      "Epoch 12, Loss: 0.3751, Train Acc: 0.8838, Train F1: 0.8301, Train AUC: 0.5634, Val Acc: 0.9163, Val F1: 0.8788, Val AUC: 0.4651\n",
      "Epoch 13, Loss: 0.3750, Train Acc: 0.8837, Train F1: 0.8300, Train AUC: 0.5645, Val Acc: 0.9160, Val F1: 0.8786, Val AUC: 0.4672\n",
      "Epoch 14, Loss: 0.3763, Train Acc: 0.8838, Train F1: 0.8301, Train AUC: 0.5666, Val Acc: 0.9158, Val F1: 0.8785, Val AUC: 0.4689\n",
      "Epoch 15, Loss: 0.3772, Train Acc: 0.8838, Train F1: 0.8301, Train AUC: 0.5696, Val Acc: 0.9160, Val F1: 0.8786, Val AUC: 0.4709\n",
      "Epoch 16, Loss: 0.3768, Train Acc: 0.8840, Train F1: 0.8302, Train AUC: 0.5731, Val Acc: 0.9162, Val F1: 0.8787, Val AUC: 0.4734\n",
      "Epoch 17, Loss: 0.3748, Train Acc: 0.8839, Train F1: 0.8300, Train AUC: 0.5762, Val Acc: 0.9165, Val F1: 0.8789, Val AUC: 0.4757\n",
      "Epoch 18, Loss: 0.3717, Train Acc: 0.8839, Train F1: 0.8299, Train AUC: 0.5786, Val Acc: 0.9171, Val F1: 0.8791, Val AUC: 0.4770\n",
      "Epoch 19, Loss: 0.3685, Train Acc: 0.8840, Train F1: 0.8299, Train AUC: 0.5802, Val Acc: 0.9171, Val F1: 0.8791, Val AUC: 0.4779\n",
      "Epoch 20, Loss: 0.3659, Train Acc: 0.8841, Train F1: 0.8300, Train AUC: 0.5814, Val Acc: 0.9172, Val F1: 0.8792, Val AUC: 0.4786\n",
      "Epoch 21, Loss: 0.3643, Train Acc: 0.8841, Train F1: 0.8299, Train AUC: 0.5826, Val Acc: 0.9178, Val F1: 0.8795, Val AUC: 0.4795\n",
      "Epoch 22, Loss: 0.3634, Train Acc: 0.8841, Train F1: 0.8299, Train AUC: 0.5839, Val Acc: 0.9178, Val F1: 0.8795, Val AUC: 0.4806\n",
      "Epoch 23, Loss: 0.3632, Train Acc: 0.8841, Train F1: 0.8299, Train AUC: 0.5857, Val Acc: 0.9180, Val F1: 0.8796, Val AUC: 0.4820\n",
      "Epoch 24, Loss: 0.3630, Train Acc: 0.8841, Train F1: 0.8299, Train AUC: 0.5878, Val Acc: 0.9180, Val F1: 0.8796, Val AUC: 0.4847\n",
      "Epoch 25, Loss: 0.3627, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.5905, Val Acc: 0.9180, Val F1: 0.8799, Val AUC: 0.4844\n",
      "Epoch 26, Loss: 0.3620, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.5933, Val Acc: 0.9180, Val F1: 0.8799, Val AUC: 0.4837\n",
      "Epoch 27, Loss: 0.3609, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.5958, Val Acc: 0.9182, Val F1: 0.8800, Val AUC: 0.4828\n",
      "Epoch 28, Loss: 0.3597, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.5976, Val Acc: 0.9183, Val F1: 0.8801, Val AUC: 0.4812\n",
      "Epoch 29, Loss: 0.3586, Train Acc: 0.8842, Train F1: 0.8301, Train AUC: 0.5987, Val Acc: 0.9185, Val F1: 0.8802, Val AUC: 0.4791\n",
      "Epoch 30, Loss: 0.3577, Train Acc: 0.8842, Train F1: 0.8301, Train AUC: 0.5993, Val Acc: 0.9183, Val F1: 0.8798, Val AUC: 0.4772\n",
      "Epoch 31, Loss: 0.3572, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.5995, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4751\n",
      "Epoch 32, Loss: 0.3572, Train Acc: 0.8843, Train F1: 0.8301, Train AUC: 0.5986, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4713\n",
      "Epoch 33, Loss: 0.3576, Train Acc: 0.8843, Train F1: 0.8301, Train AUC: 0.6004, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4684\n",
      "Epoch 34, Loss: 0.3572, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6022, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4663\n",
      "Epoch 35, Loss: 0.3566, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6039, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4648\n",
      "Epoch 36, Loss: 0.3561, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6055, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4649\n",
      "Epoch 37, Loss: 0.3556, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6067, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4662\n",
      "Epoch 38, Loss: 0.3552, Train Acc: 0.8843, Train F1: 0.8301, Train AUC: 0.6072, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4689\n",
      "Epoch 39, Loss: 0.3549, Train Acc: 0.8843, Train F1: 0.8301, Train AUC: 0.6073, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4712\n",
      "Epoch 40, Loss: 0.3548, Train Acc: 0.8843, Train F1: 0.8301, Train AUC: 0.6071, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4723\n",
      "Epoch 41, Loss: 0.3547, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.6071, Val Acc: 0.9182, Val F1: 0.8793, Val AUC: 0.4730\n",
      "Epoch 42, Loss: 0.3547, Train Acc: 0.8843, Train F1: 0.8302, Train AUC: 0.6075, Val Acc: 0.9180, Val F1: 0.8792, Val AUC: 0.4734\n",
      "Epoch 43, Loss: 0.3545, Train Acc: 0.8843, Train F1: 0.8302, Train AUC: 0.6083, Val Acc: 0.9180, Val F1: 0.8792, Val AUC: 0.4738\n",
      "Epoch 44, Loss: 0.3542, Train Acc: 0.8842, Train F1: 0.8302, Train AUC: 0.6097, Val Acc: 0.9180, Val F1: 0.8792, Val AUC: 0.4742\n",
      "Epoch 45, Loss: 0.3539, Train Acc: 0.8842, Train F1: 0.8301, Train AUC: 0.6113, Val Acc: 0.9180, Val F1: 0.8792, Val AUC: 0.4757\n",
      "Epoch 46, Loss: 0.3536, Train Acc: 0.8843, Train F1: 0.8301, Train AUC: 0.6129, Val Acc: 0.9182, Val F1: 0.8793, Val AUC: 0.4760\n",
      "Epoch 47, Loss: 0.3533, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.6142, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4756\n",
      "Epoch 48, Loss: 0.3531, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.6151, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4745\n",
      "Epoch 49, Loss: 0.3530, Train Acc: 0.8842, Train F1: 0.8300, Train AUC: 0.6159, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4727\n",
      "Epoch 50, Loss: 0.3529, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6166, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4709\n",
      "Epoch 51, Loss: 0.3527, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6173, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4686\n",
      "Epoch 52, Loss: 0.3525, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6180, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4672\n",
      "Epoch 53, Loss: 0.3524, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6186, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4672\n",
      "Epoch 54, Loss: 0.3522, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6193, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4675\n",
      "Epoch 55, Loss: 0.3521, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6199, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4678\n",
      "Epoch 56, Loss: 0.3519, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6204, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4682\n",
      "Epoch 57, Loss: 0.3518, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6209, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4688\n",
      "Epoch 58, Loss: 0.3517, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6212, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4699\n",
      "Epoch 59, Loss: 0.3515, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6216, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4711\n",
      "Epoch 60, Loss: 0.3514, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6220, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4721\n",
      "Epoch 61, Loss: 0.3512, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6226, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4732\n",
      "Epoch 62, Loss: 0.3511, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6231, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4733\n",
      "Epoch 63, Loss: 0.3510, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6237, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4727\n",
      "Epoch 64, Loss: 0.3509, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6243, Val Acc: 0.9183, Val F1: 0.8794, Val AUC: 0.4721\n",
      "Epoch 65, Loss: 0.3508, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6250, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4710\n",
      "Epoch 66, Loss: 0.3507, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6256, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4697\n",
      "Epoch 67, Loss: 0.3506, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6263, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4689\n",
      "Epoch 68, Loss: 0.3505, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6268, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4688\n",
      "Epoch 69, Loss: 0.3504, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6273, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4694\n",
      "Epoch 70, Loss: 0.3503, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6278, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4700\n",
      "Epoch 71, Loss: 0.3502, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6283, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4706\n",
      "Epoch 72, Loss: 0.3501, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6289, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4711\n",
      "Epoch 73, Loss: 0.3499, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6294, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4719\n",
      "Epoch 74, Loss: 0.3498, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6298, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4727\n",
      "Epoch 75, Loss: 0.3497, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6303, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4735\n",
      "Epoch 76, Loss: 0.3496, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6307, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4739\n",
      "Epoch 77, Loss: 0.3495, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6311, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4741\n",
      "Epoch 78, Loss: 0.3494, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6315, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4741\n",
      "Epoch 79, Loss: 0.3493, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6319, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4739\n",
      "Epoch 80, Loss: 0.3493, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6323, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4738\n",
      "Epoch 81, Loss: 0.3492, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6326, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4738\n",
      "Epoch 82, Loss: 0.3491, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6330, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4738\n",
      "Epoch 83, Loss: 0.3490, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6335, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4742\n",
      "Epoch 84, Loss: 0.3489, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6340, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4745\n",
      "Epoch 85, Loss: 0.3488, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6344, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4749\n",
      "Epoch 86, Loss: 0.3487, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6349, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4751\n",
      "Epoch 87, Loss: 0.3486, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6353, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4752\n",
      "Epoch 88, Loss: 0.3485, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6358, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4751\n",
      "Epoch 89, Loss: 0.3484, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6362, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4751\n",
      "Epoch 90, Loss: 0.3483, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6366, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4752\n",
      "Epoch 91, Loss: 0.3482, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6371, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4751\n",
      "Epoch 92, Loss: 0.3481, Train Acc: 0.8842, Train F1: 0.8299, Train AUC: 0.6375, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4753\n",
      "Epoch 93, Loss: 0.3480, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6379, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4754\n",
      "Epoch 94, Loss: 0.3479, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6383, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4754\n",
      "Epoch 95, Loss: 0.3479, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6387, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4754\n",
      "Epoch 96, Loss: 0.3478, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6391, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4754\n",
      "Epoch 97, Loss: 0.3477, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6395, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4754\n",
      "Epoch 98, Loss: 0.3476, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6399, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4755\n",
      "Epoch 99, Loss: 0.3475, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6404, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4754\n",
      "Epoch 100, Loss: 0.3474, Train Acc: 0.8843, Train F1: 0.8300, Train AUC: 0.6409, Val Acc: 0.9185, Val F1: 0.8795, Val AUC: 0.4755\n",
      "Test Accuracy: 0.9428, Test F1: 0.9155, Test AUC: 0.5201\n"
     ]
    }
   ],
   "source": [
    "# Define GCN\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "# Initialize the model\n",
    "in_channels = data.num_features\n",
    "out_channels = len(class_mapping) - 1  # Exclude unknown class\n",
    "model = GCN(in_channels, out_channels).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=0.01, \n",
    "                             weight_decay=0.0005)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function with F1-score and AUC\n",
    "def evaluate(mask, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        logits = out[mask]  # Get logits for the masked nodes\n",
    "        pred = logits.argmax(dim=1)  # Predicted class\n",
    "        prob = F.softmax(logits, dim=1)[:, 1]  # Probability for the positive class (for AUC)\n",
    "\n",
    "        # Accuracy\n",
    "        correct = (pred == labels).sum().item()\n",
    "        acc = correct / mask.size(0)\n",
    "\n",
    "        # F1-score\n",
    "        f1 = f1_score(labels.cpu().numpy(), pred.cpu().numpy(), average=\"weighted\")\n",
    "\n",
    "        # AUC (only if there are at least two classes in the mask)\n",
    "        try:\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), prob.cpu().numpy())\n",
    "        except ValueError:\n",
    "            auc = float('nan')  # AUC is undefined if only one class is present\n",
    "\n",
    "    return acc, f1, auc\n",
    "\n",
    "# Training with validation\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train()\n",
    "    train_acc, train_f1, train_auc = evaluate(train_mask, train_y)\n",
    "    val_acc, val_f1, val_auc = evaluate(val_mask, val_y)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Train AUC: {train_auc:.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_f1, test_auc = evaluate(test_mask, test_y)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}, Test AUC: {test_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
